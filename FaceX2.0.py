import sys
import cv2
import numpy as np
import insightface
from insightface.app import FaceAnalysis
from insightface.model_zoo import get_model
import time
from PyQt5.QtWidgets import QApplication, QMainWindow, QLabel, QPushButton, QVBoxLayout, QHBoxLayout, QWidget, \
    QFileDialog, QListWidget, QListWidgetItem, QScrollArea, QMessageBox, QSizePolicy, QGridLayout, QSlider
from PyQt5.QtGui import QImage, QPixmap, QIcon
from PyQt5.QtCore import QTimer, Qt, QSize
from PyQt5.QtWidgets import QLineEdit  # æ·»åŠ åˆ°ç°æœ‰çš„importsä¸­
import requests  # ç”¨äºAPIè°ƒç”¨
import json  # ç”¨äºå¤„ç†APIå“åº”
import os  # ç”¨äºæ–‡ä»¶æ“ä½œ



# import cupy as cp


def init_face_analyzer(det_size=(640, 640)):
    # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨ï¼Œä½¿ç”¨ GPU
    analyzer = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
    analyzer.prepare(ctx_id=0, det_size=det_size)
    return analyzer


def load_source_face(img_path, analyzer):
    # åŠ è½½æºå›¾åƒå¹¶æå–äººè„¸
    source_img = cv2.imread(img_path)
    if source_img is None:
        raise FileNotFoundError(f"æ— æ³•åŠ è½½æºå›¾åƒ: {img_path}")
    faces = analyzer.get(source_img)
    if not faces:
        raise ValueError("æœªåœ¨æºå›¾åƒä¸­æ£€æµ‹åˆ°äººè„¸")
    return faces[0]


def setup_camera(resolution=(320, 240), fps=30):
    # åˆå§‹åŒ–æ‘„åƒå¤´æ•è·ï¼Œå¹¶è®¾ç½®åˆ†è¾¨ç‡å’Œå¸§ç‡
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, resolution[0])
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, resolution[1])
    cap.set(cv2.CAP_PROP_FPS, fps)
    if not cap.isOpened():
        raise IOError("æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
    return cap


def swap_faces_in_frame(frame, analyzer, swapper, source_face):
    # åœ¨å¸§ä¸­è¿›è¡Œäººè„¸æ›¿æ¢
    target_faces = analyzer.get(frame)
    if target_faces:
        target_face = target_faces[0]
        # ä½¿ç”¨ GPU è¿›è¡Œäººè„¸æ›¿æ¢
        swapped_frame = swapper.get(frame, target_face, source_face, paste_back=True)
        return swapped_frame
    return frame


class FaceSwapApp(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("å®æ—¶æ¢è„¸åº”ç”¨")
        self.setGeometry(100, 100, 1200, 800)

        # åˆå§‹åŒ– InsightFace ç»„ä»¶
        self.face_analyzer = init_face_analyzer()
        self.face_swapper = get_model('inswapper_128.onnx', download=False,
                                    providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])

        # æºå›¾åƒè·¯å¾„å’Œäººè„¸
        self.source_face = None

        # æ‘„åƒå¤´åˆå§‹åŒ– - é™ä½åˆ†è¾¨ç‡ä»¥æé«˜æ€§èƒ½
        self.cap = setup_camera(resolution=(320, 240), fps=30)

        # æ˜¯å¦å¼€å¯æ¢è„¸
        self.is_swapping = False

        # å½•åˆ¶è§†é¢‘ç›¸å…³å˜é‡
        self.recording = False
        self.out = None
        self.record_start_time = None

        # å¸§å¤„ç†æ§åˆ¶
        self.frame_count = 0
        self.process_every_n_frames = 5  # å¢åŠ å¤„ç†é—´éš”åˆ°5å¸§
        self.last_processed_frame = None
        self.last_landmarks = None
        self.last_face = None
        self.processing_enabled = True
        
        # æ·»åŠ å¹³æ»‘å¤„ç†
        self.face_history = []  # å­˜å‚¨æœ€è¿‘çš„äººè„¸æ£€æµ‹ç»“æœ
        self.max_history = 10   # å¢åŠ å†å²è®°å½•é•¿åº¦
        self.landmark_history = []  # å­˜å‚¨æœ€è¿‘çš„ç‰¹å¾ç‚¹
        self.smooth_factor = 0.85   # å¢åŠ å¹³æ»‘å› å­
        self.face_detection_confidence = 0.5  # äººè„¸æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
        self.last_valid_face = None  # å­˜å‚¨æœ€åä¸€ä¸ªæœ‰æ•ˆçš„äººè„¸æ£€æµ‹ç»“æœ
        self.face_detection_fail_count = 0  # äººè„¸æ£€æµ‹å¤±è´¥è®¡æ•°
        self.max_fail_count = 10  # æœ€å¤§å¤±è´¥æ¬¡æ•°

        # é¢„è®¾çš„å››å¼ å›¾ç‰‡è·¯å¾„
        self.preset_images = [
            "pictures/img.png",
            "pictures/img_2.png",
            "pictures/img_3.png",
            "pictures/img_4.png"
        ]

        # åˆ›å»º GUI å…ƒç´ 
        self.init_ui()

        # å®šæ—¶å™¨æ›´æ–°è§†é¢‘æµ
        self.timer = QTimer(self)
        self.timer.timeout.connect(self.update_frame)
        self.timer.start(33)  # æ¯ 33 æ¯«ç§’æ›´æ–°ä¸€æ¬¡

        # æ·»åŠ é¢„è®¾å›¾ç‰‡åˆ°åˆ—è¡¨
        self.init_preset_images()

    def init_ui(self):
        # è®¾ç½®çª—å£æ ·å¼
        self.setStyleSheet("""
            QMainWindow {
                background-color: #1c1c1e;
                color: #ffffff;
            }
            QLabel {
                color: #ffffff;
                font-size: 14px;
            }
            QPushButton {
                background-color: #323236;
                border: none;
                border-radius: 10px;
                color: #ffffff;
                padding: 8px 16px;
                font-size: 14px;
            }
            QPushButton:hover {
                background-color: #3a3a3e;
            }
            QPushButton:pressed {
                background-color: #2c2c30;
            }
            QListWidget {
                background-color: #2c2c30;
                border: none;
                border-radius: 10px;
                color: #ffffff;
            }
        """)

        # ä¸»å¸ƒå±€
        main_widget = QWidget()
        self.setCentralWidget(main_widget)
        main_layout = QHBoxLayout()
        main_widget.setLayout(main_layout)

        # å·¦ä¾§ï¼šè§†é¢‘æ˜¾ç¤ºåŒºåŸŸ
        video_layout = QVBoxLayout()
        self.video_label = QLabel(self)
        self.video_label.setAlignment(Qt.AlignCenter)
        self.video_label.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        self.video_label.setStyleSheet("background-color: #2c2c30; border-radius: 15px; padding: 10px;")
        self.video_label.setMinimumSize(640, 480)  # è®¾ç½®æœ€å°å°ºå¯¸
        self.video_label.setMaximumSize(960, 720)  # è®¾ç½®æœ€å¤§å°ºå¯¸
        video_layout.addWidget(self.video_label)

        # FPS æ ‡ç­¾
        self.fps_label = QLabel("FPS: 0", self)
        self.fps_label.setAlignment(Qt.AlignCenter)
        video_layout.addWidget(self.fps_label)

        # å½•åˆ¶æ—¶é—´æ ‡ç­¾
        self.record_time_label = QLabel("", self)
        self.record_time_label.setAlignment(Qt.AlignCenter)
        self.record_time_label.hide()
        video_layout.addWidget(self.record_time_label)

        # åº•éƒ¨æŒ‰é’®å¸ƒå±€
        bottom_layout = QHBoxLayout()

        # æˆªå›¾æŒ‰é’®
        self.screenshot_button = QPushButton("ğŸ“¸ æˆªå›¾", self)
        self.screenshot_button.clicked.connect(self.take_screenshot)
        bottom_layout.addWidget(self.screenshot_button)

        # å½•åˆ¶æŒ‰é’®
        self.record_button = QPushButton("ğŸ”´ å¼€å§‹å½•åˆ¶", self)
        self.record_button.clicked.connect(self.toggle_recording)
        bottom_layout.addWidget(self.record_button)

        video_layout.addLayout(bottom_layout)
        main_layout.addLayout(video_layout)

        # ä¸­é—´ï¼šæç¤ºä¿¡æ¯åŒºåŸŸ
        face_list_layout = QVBoxLayout()
        title_label = QLabel("Tips", self)
        title_label.setStyleSheet("font-size: 20px; font-weight: bold; margin-bottom: 10px;")
        title_label.setAlignment(Qt.AlignCenter)  # è®¾ç½®æ–‡å­—å±…ä¸­
        face_list_layout.addWidget(title_label)
        
        # æ·»åŠ æç¤ºå†…å®¹
        tips_label = QLabel("1.ç‚¹å‡»å›¾åƒä½œä¸ºæ¢è„¸ç›®æ ‡ï¼Œæ»‘åŠ¨æ¢è„¸å¼€å…³ï¼›", self)
        tips_label.setStyleSheet("font-size: 16px; margin-bottom: 5px;")
        # tips_label.setAlignment(Qt.AlignCenter)  # æ³¨é‡Šæ‰å±…ä¸­å¯¹é½
        tips_label.setWordWrap(True)  # å…è®¸æ–‡æœ¬æ¢è¡Œ
        face_list_layout.addWidget(tips_label)
        
        # æ·»åŠ ç¬¬äºŒæ¡æç¤º
        tips_label2 = QLabel("2.å¯ç‚¹å‡»å›¾ç‰‡ä¸‹æ–¹æœ¬åœ°æ–‡ä»¶å¤¹ä¸­æ›´æ¢æ¢è„¸ç›®æ ‡ï¼›", self)
        tips_label2.setStyleSheet("font-size: 16px; margin-bottom: 5px;")
        # tips_label2.setAlignment(Qt.AlignCenter)  # æ³¨é‡Šæ‰å±…ä¸­å¯¹é½
        tips_label2.setWordWrap(True)
        face_list_layout.addWidget(tips_label2)
        
        # æ·»åŠ ç¬¬ä¸‰æ¡æç¤º
        tips_label3 = QLabel("3.ç‚¹å‡»è§†é¢‘ä¸‹æ–¹æŒ‰é’®ï¼Œè¿›è¡Œæˆªå›¾å’Œå½•å±ï¼›", self)
        tips_label3.setStyleSheet("font-size: 16px; margin-bottom: 5px;")
        # tips_label3.setAlignment(Qt.AlignCenter)  # æ³¨é‡Šæ‰å±…ä¸­å¯¹é½
        tips_label3.setWordWrap(True)
        face_list_layout.addWidget(tips_label3)
        
        # æ·»åŠ ç¬¬å››æ¡æç¤º
        tips_label4 = QLabel("4.å¯ä»¥é€šè¿‡è°ƒèŠ‚æ»‘åŠ¨æ¡å¯¹é¢éƒ¨è¿›è¡Œå¾®è°ƒ", self)
        tips_label4.setStyleSheet("font-size: 16px; margin-bottom: 15px;")
        # tips_label4.setAlignment(Qt.AlignCenter)  # æ³¨é‡Šæ‰å±…ä¸­å¯¹é½
        tips_label4.setWordWrap(True)
        face_list_layout.addWidget(tips_label4)

        # åˆ›å»º2x2ç½‘æ ¼å¸ƒå±€
        grid_widget = QWidget()
        grid_widget.setStyleSheet("background-color: #2c2c30; border-radius: 15px; padding: 20px;")
        grid_layout = QGridLayout(grid_widget)
        grid_layout.setSpacing(20)
        self.image_buttons = []
        self.replace_buttons = []

        for i in range(4):
            container = QWidget()
            container_layout = QVBoxLayout(container)
            container_layout.setAlignment(Qt.AlignCenter)

            # åˆ›å»ºå›¾ç‰‡æŒ‰é’®
            img_button = QPushButton()
            img_button.setFixedSize(150, 150)
            img_button.setIconSize(QSize(140, 140))
            img_button.setStyleSheet("""
                QPushButton {
                    background-color: #3a3a3e;
                    border-radius: 15px;
                }
                QPushButton:hover {
                    background-color: #444448;
                }
            """)
            img_button.clicked.connect(lambda checked, index=i: self.select_preset_image(index))
            self.image_buttons.append(img_button)
            container_layout.addWidget(img_button)

            # åˆ›å»ºæ›¿æ¢æŒ‰é’®
            replace_button = QPushButton("æ›´æ¢å›¾ç‰‡")
            replace_button.setStyleSheet("font-size: 12px; padding: 5px 10px;")
            replace_button.clicked.connect(lambda checked, index=i: self.replace_preset_image(index))
            self.replace_buttons.append(replace_button)
            container_layout.addWidget(replace_button)

            row = i // 2
            col = i % 2
            grid_layout.addWidget(container, row, col, Qt.AlignCenter)

        face_list_layout.addWidget(grid_widget)
        main_layout.addLayout(face_list_layout)

        # å³ä¾§ï¼šæ§åˆ¶é¢æ¿
        control_layout = QVBoxLayout()
        control_widget = QWidget()
        control_widget.setStyleSheet("background-color: #2c2c30; border-radius: 15px; padding: 20px;")
        control_inner_layout = QVBoxLayout(control_widget)

        # æºå›¾åƒé€‰æ‹©æŒ‰é’®
        self.select_button = QPushButton("é€‰æ‹©æºå›¾åƒ", self)
        self.select_button.clicked.connect(self.select_source_image)
        control_inner_layout.addWidget(self.select_button)

        # æ¢è„¸å¼€å…³
        switch_layout = QHBoxLayout()
        switch_label = QLabel("æ¢è„¸å¼€å…³", self)
        self.face_swap_switch = QSlider(Qt.Horizontal)
        self.face_swap_switch.setStyleSheet("""
            QSlider::groove:horizontal {
                border: 1px solid #999999;
                height: 8px;
                background: #323236;
                margin: 2px 0;
                border-radius: 4px;
            }
            QSlider::handle:horizontal {
                background: #0a84ff;
                border: 2px solid #0a84ff;
                width: 18px;
                height: 18px;
                margin: -6px 0;
                border-radius: 10px;
            }
            QSlider::handle:horizontal:hover {
                background: #40a9ff;
                border: 2px solid #40a9ff;
            }
            QSlider::sub-page:horizontal {
                background: #0a84ff;
                border-radius: 4px;
            }
            QSlider::add-page:horizontal {
                background: #323236;
                border-radius: 4px;
            }
        """)
        self.face_swap_switch.setFixedWidth(80)
        self.face_swap_switch.setMinimum(0)
        self.face_swap_switch.setMaximum(1)
        self.face_swap_switch.valueChanged.connect(self.toggle_face_swap)
        switch_layout.addWidget(switch_label)
        switch_layout.addWidget(self.face_swap_switch)
        control_inner_layout.addLayout(switch_layout)

        # æ·»åŠ ä¸‰ä¸ªæ–°çš„æ»‘åŠ¨æ¡
        # æ»‘åŠ¨æ¡1
        slider1_layout = QHBoxLayout()
        slider1_label = QLabel("è„¸å®½", self)
        self.slider1 = QSlider(Qt.Horizontal)
        self.slider1.setStyleSheet(self.face_swap_switch.styleSheet())  # ä½¿ç”¨ç›¸åŒçš„æ ·å¼
        self.slider1.setFixedWidth(200)  # ä¿®æ”¹å®½åº¦ä»80åˆ°200
        self.slider1.setMinimum(0)
        self.slider1.setMaximum(100)
        self.slider1.setValue(50)  # è®¾ç½®é»˜è®¤å€¼ä¸º50
        self.slider1.valueChanged.connect(self.update_parameters)  # æ·»åŠ å€¼å˜åŒ–çš„è¿æ¥
        slider1_layout.addWidget(slider1_label)
        slider1_layout.addWidget(self.slider1)
        control_inner_layout.addLayout(slider1_layout)

        # æ»‘åŠ¨æ¡2
        slider2_layout = QHBoxLayout()
        slider2_label = QLabel("çœ¼é•¿", self)
        self.slider2 = QSlider(Qt.Horizontal)
        self.slider2.setStyleSheet(self.face_swap_switch.styleSheet())
        self.slider2.setFixedWidth(200)
        self.slider2.setMinimum(30)
        self.slider2.setMaximum(70)
        self.slider2.setValue(50)
        self.slider2.valueChanged.connect(self.update_parameters)
        slider2_layout.addWidget(slider2_label)
        slider2_layout.addWidget(self.slider2)
        control_inner_layout.addLayout(slider2_layout)

        # æ»‘åŠ¨æ¡3
        slider3_layout = QHBoxLayout()
        slider3_label = QLabel("çœ¼é«˜", self)
        self.slider3 = QSlider(Qt.Horizontal)
        self.slider3.setStyleSheet(self.face_swap_switch.styleSheet())
        self.slider3.setFixedWidth(200)
        self.slider3.setMinimum(30)
        self.slider3.setMaximum(70)
        self.slider3.setValue(50)
        self.slider3.valueChanged.connect(self.update_parameters)
        slider3_layout.addWidget(slider3_label)
        slider3_layout.addWidget(self.slider3)
        control_inner_layout.addLayout(slider3_layout)

        # æºå›¾åƒæ˜¾ç¤ºåŒºåŸŸ
        self.image_label = QLabel("æºå›¾åƒ", self)
        self.image_label.setAlignment(Qt.AlignCenter)
        self.image_label.setStyleSheet("background-color: #323236; border-radius: 10px; padding: 10px;")
        self.image_label.setMinimumSize(160, 120)
        control_inner_layout.addWidget(self.image_label)

        control_layout.addWidget(control_widget)
        main_layout.addLayout(control_layout)

        # è®¾ç½®å¸ƒå±€æ¯”ä¾‹
        main_layout.setStretch(0, 2)  # è§†é¢‘åŒºåŸŸ
        main_layout.setStretch(1, 1)  # å¤‡é€‰å›¾åƒåŒºåŸŸ
        main_layout.setStretch(2, 1)  # æ§åˆ¶é¢æ¿

    def toggle_recording(self):
        if self.recording:
            self.recording = False
            self.record_button.setText("ğŸ”´ å¼€å§‹å½•åˆ¶")
            self.record_time_label.hide()
            self.out.release()
            if hasattr(self, 'record_timer'):
                self.record_timer.stop()
            msg_box = QMessageBox(self)
            msg_box.setWindowTitle("å½•åˆ¶å®Œæˆ")
            msg_box.setText(f"è§†é¢‘å·²ä¿å­˜ä¸º: {self.current_video_path}")
            msg_box.setStyleSheet("""
                QMessageBox {
                    background-color: #323236;
                }
                QMessageBox QLabel {
                    color: #ffffff;
                }
                QPushButton {
                    background-color: #0a84ff;
                    border: none;
                    border-radius: 5px;
                    color: #ffffff;
                    padding: 5px 15px;
                }
                QPushButton:hover {
                    background-color: #40a9ff;
                }
            """)
            msg_box.exec_()
        else:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            timestamp = int(time.time())
            self.current_video_path = f"recording_{timestamp}.mp4"
            self.out = cv2.VideoWriter(self.current_video_path, fourcc, 30.0, (640, 480))
            self.recording = True
            self.record_button.setText("â¹ï¸ åœæ­¢å½•åˆ¶")
            self.record_time_label.show()
            self.record_start_time = time.time()
            self.record_timer = QTimer(self)
            self.record_timer.timeout.connect(self.update_record_time)
            self.record_timer.start(1000)  # æ›´æ–°å½•åˆ¶æ—¶é—´æ¯ç§’ä¸€æ¬¡

    def update_record_time(self):
        elapsed_time = int(time.time() - self.record_start_time)
        minutes = elapsed_time // 60
        seconds = elapsed_time % 60
        self.record_time_label.setText(f"å½•åˆ¶æ—¶é—´: {minutes:02}:{seconds:02}")

    def take_screenshot(self):
        # è·å–å½“å‰è§†é¢‘å¸§
        ret, frame = self.cap.read()
        if ret:
            # å¦‚æœå¼€å¯æ¢è„¸ä¸”æœ‰æºå›¾åƒï¼Œåˆ™æ‰§è¡Œæ¢è„¸
            if self.is_swapping and self.source_face is not None:
                frame = swap_faces_in_frame(frame, self.face_analyzer, self.face_swapper, self.source_face)

            timestamp = int(time.time())
            screenshot_path = f"screenshot_{timestamp}.png"
            cv2.imwrite(screenshot_path, frame)
            msg_box = QMessageBox(self)
            msg_box.setWindowTitle("æˆªå›¾æˆåŠŸ")
            msg_box.setText(f"æˆªå›¾å·²ä¿å­˜ä¸º: {screenshot_path}")
            msg_box.setStyleSheet("""
                QMessageBox {
                    background-color: #323236;
                }
                QMessageBox QLabel {
                    color: #ffffff;
                }
                QPushButton {
                    background-color: #0a84ff;
                    border: none;
                    border-radius: 5px;
                    color: #ffffff;
                    padding: 5px 15px;
                }
                QPushButton:hover {
                    background-color: #40a9ff;
                }
            """)
            msg_box.exec_()

    def select_source_image(self):
        # æ‰“å¼€æ–‡ä»¶é€‰æ‹©å¯¹è¯æ¡†
        file_path, _ = QFileDialog.getOpenFileName(self, "é€‰æ‹©æºå›¾åƒ", "", "Image Files (*.png *.jpg *.jpeg)")
        if file_path:
            try:
                # åŠ è½½æºå›¾åƒå¹¶æå–äººè„¸
                self.source_face = load_source_face(file_path, self.face_analyzer)

                # æ˜¾ç¤ºæºå›¾åƒ
                source_img = cv2.imread(file_path)
                source_img = cv2.cvtColor(source_img, cv2.COLOR_BGR2RGB)
                h, w, ch = source_img.shape
                bytes_per_line = ch * w
                q_img = QImage(source_img.data, w, h, bytes_per_line, QImage.Format_RGB888)
                pixmap = QPixmap.fromImage(q_img)
                scaled_pixmap = pixmap.scaled(self.image_label.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation)
                self.image_label.setPixmap(scaled_pixmap)

                print(f"æˆåŠŸåŠ è½½æºå›¾åƒ: {file_path}")
            except Exception as e:
                print(f"åŠ è½½æºå›¾åƒå¤±è´¥: {str(e)}")

    def toggle_face_swap(self, value):
        if value == 0:
            self.is_swapping = False
            # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
            self.last_processed_frame = None
            self.last_landmarks = None
            self.last_face = None
            self.landmark_history = []
            self.face_detection_fail_count = 0
            print("æ¢è„¸çŠ¶æ€: å…³é—­")
        else:
            if self.source_face is None:
                msg_box = QMessageBox(self)
                msg_box.setWindowTitle("æç¤º")
                msg_box.setText("è¯·é€‰æ‹©æ¢è„¸ç›®æ ‡äººç‰©å›¾åƒ")
                msg_box.setStyleSheet("""
                    QMessageBox {
                        background-color: #323236;
                    }
                    QMessageBox QLabel {
                        color: #ffffff;
                    }
                    QPushButton {
                        background-color: #0a84ff;
                        border: none;
                        border-radius: 5px;
                        color: #ffffff;
                        padding: 5px 15px;
                    }
                    QPushButton:hover {
                        background-color: #40a9ff;
                    }
                """)
                msg_box.exec_()
                self.face_swap_switch.setValue(0)
                return
            self.is_swapping = True
            # æ¸…é™¤ç¼“å­˜
            self.last_processed_frame = None
            self.last_landmarks = None
            self.last_face = None
            self.landmark_history = []  # æ¸…é™¤ç‰¹å¾ç‚¹å†å²
            self.face_detection_fail_count = 0  # é‡ç½®å¤±è´¥è®¡æ•°
            print("æ¢è„¸çŠ¶æ€: å¼€å¯")

    def select_preset_image(self, index):
        if index < len(self.preset_images):
            try:
                self.source_face = load_source_face(self.preset_images[index], self.face_analyzer)

                # æ˜¾ç¤ºæºå›¾åƒ
                source_img = cv2.imread(self.preset_images[index])
                source_img = cv2.cvtColor(source_img, cv2.COLOR_BGR2RGB)
                h, w, ch = source_img.shape
                bytes_per_line = ch * w
                q_img = QImage(source_img.data, w, h, bytes_per_line, QImage.Format_RGB888)
                pixmap = QPixmap.fromImage(q_img)
                scaled_pixmap = pixmap.scaled(self.image_label.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation)
                self.image_label.setPixmap(scaled_pixmap)

                print(f"é€‰æ‹©äº†é¢„è®¾å›¾åƒ: {self.preset_images[index]}")
            except Exception as e:
                print(f"é€‰æ‹©é¢„è®¾å›¾åƒå¤±è´¥: {str(e)}")

    def replace_preset_image(self, index):
        if index < len(self.preset_images):
            file_path, _ = QFileDialog.getOpenFileName(self, "é€‰æ‹©æ–°å›¾åƒ", "", "Image Files (*.png *.jpg *.jpeg)")
            if file_path:
                try:
                    # åŠ è½½æ–°å›¾åƒå¹¶æå–äººè„¸
                    new_face = load_source_face(file_path, self.face_analyzer)

                    # æ›´æ–°é¢„è®¾å›¾åƒè·¯å¾„
                    self.preset_images[index] = file_path

                    # æ›´æ–°æŒ‰é’®å›¾æ ‡
                    img = cv2.imread(file_path)
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    h, w, ch = img.shape
                    bytes_per_line = ch * w
                    q_img = QImage(img.data, w, h, bytes_per_line, QImage.Format_RGB888)
                    icon = QIcon(QPixmap.fromImage(q_img).scaled(140, 140, Qt.KeepAspectRatio, Qt.SmoothTransformation))
                    self.image_buttons[index].setIcon(icon)

                    print(f"æ›¿æ¢äº†é¢„è®¾å›¾åƒ: {file_path}")
                except Exception as e:
                    print(f"æ›¿æ¢é¢„è®¾å›¾åƒå¤±è´¥: {str(e)}")

    def init_preset_images(self):
        for i, img_path in enumerate(self.preset_images):
            if i >= 4:
                break
            try:
                img = cv2.imread(img_path)
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                h, w, ch = img.shape
                bytes_per_line = ch * w
                q_img = QImage(img.data, w, h, bytes_per_line, QImage.Format_RGB888)
                icon = QIcon(QPixmap.fromImage(q_img).scaled(140, 140, Qt.KeepAspectRatio, Qt.SmoothTransformation))
                self.image_buttons[i].setIcon(icon)
            except Exception as e:
                print(f"åŠ è½½é¢„è®¾å›¾åƒå¤±è´¥: {str(e)}")

    def smooth_landmarks(self, new_landmarks):
        """å¹³æ»‘ç‰¹å¾ç‚¹"""
        if not self.landmark_history:
            self.landmark_history = [new_landmarks] * self.max_history
            return new_landmarks
        
        # æ›´æ–°å†å²è®°å½•
        self.landmark_history.pop(0)
        self.landmark_history.append(new_landmarks)
        
        # è®¡ç®—å¹³æ»‘åçš„ç‰¹å¾ç‚¹
        smoothed = np.zeros_like(new_landmarks)
        for landmarks in self.landmark_history:
            smoothed += landmarks
        smoothed /= len(self.landmark_history)
        
        # åº”ç”¨å¹³æ»‘å› å­
        result = self.smooth_factor * smoothed + (1 - self.smooth_factor) * new_landmarks
        return result

    def update_frame(self):
        try:
            ret, frame = self.cap.read()
            if not ret:
                return

            # é™ä½å¤„ç†åˆ†è¾¨ç‡
            small_frame = cv2.resize(frame, (320, 240))
            
            # å¸§è®¡æ•°
            self.frame_count += 1
            
            # æ¯Nå¸§å¤„ç†ä¸€æ¬¡
            if self.frame_count % self.process_every_n_frames == 0 and self.processing_enabled:
                # å¦‚æœå¼€å¯æ¢è„¸ä¸”æœ‰æºå›¾åƒï¼Œåˆ™æ‰§è¡Œæ¢è„¸
                if self.is_swapping and self.source_face is not None:
                    try:
                        # äººè„¸æ£€æµ‹
                        faces = self.face_analyzer.get(small_frame)
                        
                        if len(faces) > 0 and faces[0].det_score > self.face_detection_confidence:
                            # è·å–äººè„¸ç‰¹å¾ç‚¹
                            face = faces[0]
                            
                            # æ‰“å°è°ƒè¯•ä¿¡æ¯
                            print("æ£€æµ‹åˆ°äººè„¸ï¼Œç½®ä¿¡åº¦:", face.det_score)
                            print("äººè„¸æ¡†:", face.bbox)
                            
                            # è·å–ç‰¹å¾ç‚¹
                            if hasattr(face, 'kps'):
                                landmarks = face.kps
                                print("ä½¿ç”¨kpsç‰¹å¾ç‚¹:", landmarks.shape)
                            elif hasattr(face, 'landmark'):
                                landmarks = face.landmark
                                print("ä½¿ç”¨landmarkç‰¹å¾ç‚¹:", landmarks.shape)
                            else:
                                print("æœªæ‰¾åˆ°ç‰¹å¾ç‚¹å±æ€§")
                                landmarks = None
                            
                            if landmarks is not None and len(landmarks) > 0:
                                # å¹³æ»‘ç‰¹å¾ç‚¹
                                smoothed_landmarks = self.smooth_landmarks(landmarks)
                                
                                # æ›´æ–°æœ‰æ•ˆäººè„¸
                                self.last_valid_face = face
                                self.face_detection_fail_count = 0
                                
                                # æ‰§è¡Œæ¢è„¸
                                frame = swap_faces_in_frame(small_frame, self.face_analyzer, self.face_swapper, self.source_face)
                                
                                # ç¼“å­˜ç»“æœ
                                self.last_processed_frame = frame.copy()
                                self.last_landmarks = smoothed_landmarks
                                self.last_face = face
                            else:
                                print("æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„ç‰¹å¾ç‚¹")
                                self.face_detection_fail_count += 1
                        else:
                            # äººè„¸æ£€æµ‹å¤±è´¥
                            self.face_detection_fail_count += 1
                            print("äººè„¸æ£€æµ‹å¤±è´¥æˆ–ç½®ä¿¡åº¦ä¸è¶³")
                            
                        # å¦‚æœå¤±è´¥æ¬¡æ•°æœªè¶…è¿‡é˜ˆå€¼ï¼Œä½¿ç”¨ä¸Šä¸€å¸§çš„ç»“æœ
                        if self.face_detection_fail_count < self.max_fail_count and self.last_processed_frame is not None:
                            frame = self.last_processed_frame
                        else:
                            frame = small_frame
                            self.last_processed_frame = None
                            self.last_landmarks = None
                            self.last_face = None
                    except Exception as e:
                        print(f"å¤„ç†äººè„¸æ—¶å‡ºé”™: {str(e)}")
                        frame = small_frame
                else:
                    frame = small_frame
            else:
                # ä½¿ç”¨ç¼“å­˜çš„ç»“æœ
                if self.last_processed_frame is not None:
                    frame = self.last_processed_frame
                else:
                    frame = small_frame

            # å¦‚æœæ£€æµ‹åˆ°äººè„¸ï¼Œåº”ç”¨ç¾é¢œæ•ˆæœ
            if self.last_landmarks is not None:
                try:
                    # è·å–æ»‘åŠ¨æ¡çš„å€¼
                    face_strength = (self.slider1.value() - 50) / 50.0
                    # ä¿®æ”¹çœ¼ç›ç¼©æ”¾ç³»æ•°çš„è®¡ç®—æ–¹å¼ï¼Œå¢åŠ å˜å½¢èŒƒå›´
                    eye_width = 1.0 + (self.slider2.value() - 50) / 50.0 * 1.0  # èŒƒå›´ä»0.0åˆ°2.0
                    eye_height = 1.0 + (self.slider3.value() - 50) / 50.0 * 1.0  # èŒƒå›´ä»0.0åˆ°2.0

                    print(f"çœ¼ç›å˜å½¢å‚æ•°: width={eye_width}, height={eye_height}")
                    # å¤„ç†å›¾åƒï¼ˆåŒ…å«ç¾é¢œæ•ˆæœï¼‰
                    processed_frame = process_image(frame, self.last_landmarks, face_strength, eye_width, eye_height)
                except Exception as e:
                    print(f"åº”ç”¨ç¾é¢œæ•ˆæœæ—¶å‡ºé”™: {str(e)}")
                    processed_frame = frame
            else:
                processed_frame = frame

            # å¦‚æœæ­£åœ¨å½•åˆ¶ï¼Œä¿å­˜å¸§
            if self.recording:
                # å°†å¤„ç†åçš„å¸§æ”¾å¤§åˆ°åŸå§‹å¤§å°
                full_size_frame = cv2.resize(processed_frame, (640, 480))
                self.out.write(full_size_frame)

            # è½¬æ¢ä¸ºQtå›¾åƒæ ¼å¼å¹¶æ˜¾ç¤º
            h, w, ch = processed_frame.shape
            bytes_per_line = ch * w
            qt_image = QImage(processed_frame.data, w, h, bytes_per_line, QImage.Format_BGR888)
            pixmap = QPixmap.fromImage(qt_image)
            scaled_pixmap = pixmap.scaled(self.video_label.size(), Qt.KeepAspectRatio)
            self.video_label.setPixmap(scaled_pixmap)

            # æ˜¾ç¤º FPS
            current_time = time.time()
            if hasattr(self, 'last_time') and current_time - self.last_time > 1.0:
                self.fps = getattr(self, 'frame_count', 0)
                self.frame_count = 0
                self.last_time = current_time
            self.frame_count = getattr(self, 'frame_count', 0) + 1
            self.fps_label.setText(f"FPS: {getattr(self, 'fps', 0)}")

        except Exception as e:
            print(f"æ›´æ–°å¸§æ—¶å‡ºé”™: {str(e)}")
            # ä¸è¦è‡ªåŠ¨å…³é—­æ¢è„¸å¼€å…³
            # self.is_swapping = False
            # self.face_swap_switch.setValue(0)

    def closeEvent(self, event):
        # é‡Šæ”¾èµ„æº
        self.cap.release()
        if self.out is not None:
            self.out.release()
        event.accept()

    def update_parameters(self):
        # è·å–å½“å‰æºå›¾åƒè·¯å¾„
        current_source = "æ— æºå›¾åƒ"
        if hasattr(self, 'source_face') and self.source_face is not None:
            # å¦‚æœæ˜¯é¢„è®¾å›¾åƒï¼Œæ˜¾ç¤ºé¢„è®¾å›¾åƒè·¯å¾„
            for preset_path in self.preset_images:
                if os.path.exists(preset_path):
                    try:
                        img = cv2.imread(preset_path)
                        if img is not None:
                            faces = self.face_analyzer.get(img)
                            if faces and np.array_equal(faces[0].embedding, self.source_face.embedding):
                                current_source = preset_path
                                break
                    except:
                        continue

        # è·å–æ»‘åŠ¨æ¡çš„å€¼
        face_width = self.slider1.value()
        eye_length = self.slider2.value()
        eye_height = self.slider3.value()

        # åœ¨ç»ˆç«¯è¾“å‡ºä¿¡æ¯
        print("\nå½“å‰å‚æ•°çŠ¶æ€ï¼š")
        print(f"æºå›¾åƒè·¯å¾„: {current_source}")
        print(f"è„¸å®½å‚æ•°: {face_width}")
        print(f"çœ¼é•¿å‚æ•°: {eye_length}")
        print(f"çœ¼é«˜å‚æ•°: {eye_height}")


import cv2
import dlib
import numpy as np
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import spsolve


def load_image(path):
    """åŠ è½½å¹¶éªŒè¯å›¾åƒ"""
    img = cv2.imread(path)
    if img is None:
        raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒï¼š{path}")
    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)


def hist_match(source, template):
    """ç›´æ–¹å›¾åŒ¹é…å®ç°ï¼ˆCPUç‰ˆæœ¬ï¼‰"""
    src_hist = cv2.calcHist([source], [0], None, [256], [0, 256])
    tgt_hist = cv2.calcHist([template], [0], None, [256], [0, 256])
    src_cdf = np.cumsum(src_hist)
    tgt_cdf = np.cumsum(tgt_hist)
    src_cdf_normalized = src_cdf / src_cdf[-1]
    tgt_cdf_normalized = tgt_cdf / tgt_cdf[-1]
    lut = np.interp(src_cdf_normalized, tgt_cdf_normalized, np.arange(256))
    return cv2.LUT(source, lut.astype(np.uint8))


def adjust_lighting(src, target):
    """å…‰ç…§ä¸€è‡´æ€§è°ƒæ•´ï¼ˆCPUç‰ˆæœ¬ï¼‰"""
    result = np.zeros_like(src)
    for i in range(3):
        result[:, :, i] = hist_match(src[:, :, i], target[:, :, i])
    return result


def enlarge_eyes(img, landmarks, scale_x=1.0, scale_y=1.0):
    """æ”¹è¿›çš„å¤§çœ¼æ•ˆæœï¼ˆCPUç‰ˆæœ¬ï¼‰"""
    result = img.copy()

    for eye_points in [range(36, 42), range(42, 48)]:
        points = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in eye_points])
        center = np.mean(points, axis=0).astype(int)

        x_radius = int(np.max(np.abs(points[:, 0] - center[0])) * 2.5)
        y_radius = int(np.max(np.abs(points[:, 1] - center[1])) * 2.5)

        h, w = img.shape[:2]
        x = np.arange(w)
        y = np.arange(h)
        X, Y = np.meshgrid(x, y)

        dx = X - center[0]
        dy = Y - center[1]
        dist = np.sqrt((dx / x_radius) ** 2 + (dy / y_radius) ** 2)

        pupil_radius = 0.3
        mask = (dist <= 1.0) & (dist > pupil_radius)

        strength_x = np.zeros_like(dist)
        strength_y = np.zeros_like(dist)
        strength_x[mask] = (1 - (dist[mask] - pupil_radius) / (1.0 - pupil_radius)) * (scale_x - 1)
        strength_y[mask] = (1 - (dist[mask] - pupil_radius) / (1.0 - pupil_radius)) * (scale_y - 1)

        map_x = (center[0] + dx * (1 + strength_x)).astype(np.float32)
        map_y = (center[1] + dy * (1 + strength_y)).astype(np.float32)

        warped = cv2.remap(result, map_x, map_y, cv2.INTER_LANCZOS4)

        blend_mask = np.zeros((h, w), dtype=np.float32)
        cv2.ellipse(blend_mask,
                    center=tuple(center),
                    axes=(x_radius, y_radius),
                    angle=0,
                    startAngle=0,
                    endAngle=360,
                    color=1.0,
                    thickness=-1)

        blend_mask = cv2.GaussianBlur(blend_mask, (51, 51), min(x_radius, y_radius) / 3)
        blend_mask = np.dstack([blend_mask] * 3)

        result = (warped * blend_mask + result * (1 - blend_mask)).astype(np.uint8)

    return result


def create_mls_grid(shape, src_points, dst_points):
    """ä¼˜åŒ–çš„MLSç½‘æ ¼å˜å½¢æ˜ å°„ï¼ˆCPUç‰ˆæœ¬ï¼‰"""
    h, w = shape[:2]
    Y, X = np.indices((h, w))

    grid_points = np.stack([X, Y], axis=-1).reshape(-1, 2).astype(np.float32)

    src_points = np.array(src_points, dtype=np.float32)
    dst_points = np.array(dst_points, dtype=np.float32)

    weights = np.zeros((len(grid_points), len(src_points)), dtype=np.float32)
    for i, p in enumerate(src_points):
        diff = grid_points - p
        dist = np.sqrt(np.sum(diff ** 2, axis=1)) + 1e-8
        weights[:, i] = 1 / dist

    weights /= weights.sum(axis=1, keepdims=True)

    delta = dst_points - src_points
    grid_x = X + np.sum(weights * delta[:, 0], axis=1).reshape(h, w)
    grid_y = Y + np.sum(weights * delta[:, 1], axis=1).reshape(h, w)

    return np.dstack((grid_x, grid_y))


def slim_face(img, landmarks, strength=0.3):
    """æ”¹è¿›çš„ç˜¦è„¸æ•ˆæœï¼ˆCPUç‰ˆæœ¬ï¼‰"""
    jaw_src = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(0, 17)]
    center_x = img.shape[1] // 2

    jaw_dst = [(x - (x - center_x) * strength, y) for x, y in jaw_src]

    grid = create_mls_grid(img.shape, jaw_src, jaw_dst)

    result = cv2.remap(img, grid[:, :, 0].astype(np.float32),
                       grid[:, :, 1].astype(np.float32),
                       cv2.INTER_LANCZOS4)

    return result


def process_image(img, landmarks, face_strength, eye_scale_x, eye_scale_y):
    """å¤„ç†å›¾åƒçš„ä¸»å‡½æ•°ï¼ˆCPUç‰ˆæœ¬ï¼‰"""
    try:
        result = img.copy()

        # æ‰“å°æ‰€æœ‰ç‰¹å¾ç‚¹ï¼Œç”¨äºè°ƒè¯•
        print("æ‰€æœ‰ç‰¹å¾ç‚¹:", landmarks)

        # ç¡®ä¿landmarksæ˜¯numpyæ•°ç»„
        landmarks = np.array(landmarks)
        if landmarks.size == 0:
            print("ç‰¹å¾ç‚¹ä¸ºç©º")
            return img

        # æ‰“å°ç‰¹å¾ç‚¹å½¢çŠ¶
        print("ç‰¹å¾ç‚¹å½¢çŠ¶:", landmarks.shape)

        # æ ¹æ®ç‰¹å¾ç‚¹å½¢çŠ¶è°ƒæ•´å¤„ç†æ–¹å¼
        if landmarks.shape[0] == 5:  # å¦‚æœæ˜¯5ç‚¹ç‰¹å¾ç‚¹
            # ä½¿ç”¨5ç‚¹ç‰¹å¾ç‚¹è¿›è¡Œçœ¼ç›å¤„ç†
            left_eye = landmarks[0]  # å·¦çœ¼ä¸­å¿ƒ
            right_eye = landmarks[1]  # å³çœ¼ä¸­å¿ƒ
            nose = landmarks[2]  # é¼»å­
            left_mouth = landmarks[3]  # å·¦å˜´è§’
            right_mouth = landmarks[4]  # å³å˜´è§’

            # è®¡ç®—é¢éƒ¨ä¸­å¿ƒç‚¹
            eye_center = (left_eye + right_eye) / 2  # ä¸¤çœ¼ä¸­å¿ƒç‚¹
            face_center = (eye_center + nose) / 2  # é¢éƒ¨ä¸­å¿ƒç‚¹ï¼ˆçœ¼ç›ä¸­å¿ƒç‚¹å’Œé¼»å­çš„ä¸­ç‚¹ï¼‰

            # è®¡ç®—è„¸å®½
            face_width = np.linalg.norm(left_eye - right_eye) * 3.0

            # åˆ›å»ºå˜å½¢ç½‘æ ¼
            h, w = img.shape[:2]
            Y, X = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')

            # è®¡ç®—æ¯ä¸ªç‚¹åˆ°é¢éƒ¨ä¸­å¿ƒç‚¹çš„è·ç¦»
            points = np.stack([X, Y], axis=-1)
            face_center_points = np.tile(face_center, (h, w, 1))
            vectors = points - face_center_points
            
            # è®¡ç®—åˆ°é¢éƒ¨ä¸­å¿ƒç‚¹çš„è·ç¦»
            dist = np.sqrt(np.sum(vectors ** 2, axis=-1))
            dist = dist / face_width  # å½’ä¸€åŒ–è·ç¦»
            
            # åˆ›å»ºå˜å½¢æ©ç 
            mask = dist < 1.2

            if np.any(mask):
                # è®¡ç®—å˜å½¢å¼ºåº¦
                strength = np.zeros_like(dist)
                strength[mask] = (1 - dist[mask]) * face_strength * 1.5
                
                # è®¡ç®—å˜å½¢æ–¹å‘ï¼ˆä»é¢éƒ¨ä¸­å¿ƒç‚¹å‘å¤–ï¼‰
                direction = vectors[mask] / (np.linalg.norm(vectors[mask], axis=-1, keepdims=True) + 1e-8)
                
                # åº”ç”¨å˜å½¢
                dx = direction[:, 0] * strength[mask] * 2.0
                dy = direction[:, 1] * strength[mask] * 0.5
                
                map_x = X[mask] + dx
                map_y = Y[mask] + dy

                # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
                map_x = np.clip(map_x, 0, w-1)
                map_y = np.clip(map_y, 0, h-1)

                # åˆ›å»ºå®Œæ•´çš„æ˜ å°„
                full_map_x = X.copy()
                full_map_y = Y.copy()
                full_map_x[mask] = map_x
                full_map_y[mask] = map_y

                # åˆ›å»ºå¹³æ»‘è¿‡æ¸¡çš„æ©ç 
                smooth_mask = np.zeros_like(dist, dtype=np.float32)
                smooth_mask[mask] = 1.0
                
                # ä½¿ç”¨é«˜æ–¯æ¨¡ç³Šåˆ›å»ºå¹³æ»‘è¿‡æ¸¡
                smooth_mask = cv2.GaussianBlur(smooth_mask, (51, 51), 15)
                
                # åº”ç”¨å˜å½¢
                warped = cv2.remap(result, full_map_x.astype(np.float32), full_map_y.astype(np.float32), cv2.INTER_LINEAR)
                
                # ä½¿ç”¨å¹³æ»‘æ©ç è¿›è¡Œæ··åˆ
                smooth_mask = np.dstack([smooth_mask] * 3)
                result = (warped * smooth_mask + result * (1 - smooth_mask)).astype(np.uint8)

            # è®¡ç®—çœ¼ç›åŠå¾„
            eye_radius = np.linalg.norm(left_eye - right_eye) * 0.3

            # å¤„ç†å·¦çœ¼
            center = left_eye
            x_radius = y_radius = eye_radius

            # è®¡ç®—å˜å½¢å¼ºåº¦
            dx = X - center[0]
            dy = Y - center[1]
            dist = np.sqrt((dx / x_radius) ** 2 + (dy / y_radius) ** 2)

            pupil_radius = 0.2
            mask = (dist <= 1.0) & (dist > pupil_radius)

            if np.any(mask):
                strength_x = np.zeros_like(dist)
                strength_y = np.zeros_like(dist)
                
                mask_coef = (1 - (dist[mask] - pupil_radius) / (1.0 - pupil_radius))
                strength_x[mask] = mask_coef * (eye_scale_x - 1) * 2.0
                strength_y[mask] = mask_coef * (eye_scale_y - 1) * 2.0

                map_x = X + dx * strength_x
                map_y = Y + dy * strength_y

                map_x = np.clip(map_x, 0, w-1)
                map_y = np.clip(map_y, 0, h-1)

                # åˆ›å»ºçœ¼ç›åŒºåŸŸçš„å¹³æ»‘æ©ç 
                eye_mask = np.zeros_like(dist, dtype=np.float32)
                eye_mask[mask] = 1.0
                eye_mask = cv2.GaussianBlur(eye_mask, (31, 31), 10)
                
                # åº”ç”¨çœ¼ç›å˜å½¢
                warped = cv2.remap(result, map_x.astype(np.float32), map_y.astype(np.float32), cv2.INTER_LINEAR)
                
                # ä½¿ç”¨å¹³æ»‘æ©ç è¿›è¡Œæ··åˆ
                eye_mask = np.dstack([eye_mask] * 3)
                result = (warped * eye_mask + result * (1 - eye_mask)).astype(np.uint8)

            # å¤„ç†å³çœ¼
            center = right_eye
            dx = X - center[0]
            dy = Y - center[1]
            dist = np.sqrt((dx / x_radius) ** 2 + (dy / y_radius) ** 2)

            mask = (dist <= 1.0) & (dist > pupil_radius)

            if np.any(mask):
                strength_x = np.zeros_like(dist)
                strength_y = np.zeros_like(dist)
                
                mask_coef = (1 - (dist[mask] - pupil_radius) / (1.0 - pupil_radius))
                strength_x[mask] = mask_coef * (eye_scale_x - 1) * 2.0
                strength_y[mask] = mask_coef * (eye_scale_y - 1) * 2.0

                map_x = X + dx * strength_x
                map_y = Y + dy * strength_y

                map_x = np.clip(map_x, 0, w-1)
                map_y = np.clip(map_y, 0, h-1)

                # åˆ›å»ºçœ¼ç›åŒºåŸŸçš„å¹³æ»‘æ©ç 
                eye_mask = np.zeros_like(dist, dtype=np.float32)
                eye_mask[mask] = 1.0
                eye_mask = cv2.GaussianBlur(eye_mask, (31, 31), 10)
                
                # åº”ç”¨çœ¼ç›å˜å½¢
                warped = cv2.remap(result, map_x.astype(np.float32), map_y.astype(np.float32), cv2.INTER_LINEAR)
                
                # ä½¿ç”¨å¹³æ»‘æ©ç è¿›è¡Œæ··åˆ
                eye_mask = np.dstack([eye_mask] * 3)
                result = (warped * eye_mask + result * (1 - eye_mask)).astype(np.uint8)

        else:  # å¦‚æœæ˜¯68ç‚¹ç‰¹å¾ç‚¹
            # ä½¿ç”¨åŸæœ‰çš„68ç‚¹ç‰¹å¾ç‚¹å¤„ç†æ–¹å¼
            jaw_src = landmarks[0:17]
            center_x = img.shape[1] // 2
            jaw_dst = np.array([(x - (x - center_x) * face_strength * 1.5, y) for x, y in jaw_src])

            h, w = img.shape[:2]
            Y, X = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')
            grid_points = np.stack([X, Y], axis=-1).reshape(-1, 2)

            weights = np.zeros((len(grid_points), len(jaw_src)))
            for i, p in enumerate(jaw_src):
                diff = grid_points - p
                dist = np.sqrt(np.sum(diff ** 2, axis=1)) + 1e-8
                weights[:, i] = 1 / dist

            weights /= weights.sum(axis=1, keepdims=True)

            delta = jaw_dst - jaw_src
            grid_x = X + np.sum(weights * delta[:, 0], axis=1).reshape(h, w)
            grid_y = Y + np.sum(weights * delta[:, 1], axis=1).reshape(h, w)

            # åˆ›å»ºä¸‹é¢Œçº¿åŒºåŸŸçš„å¹³æ»‘æ©ç 
            jaw_mask = np.zeros((h, w), dtype=np.float32)
            for i in range(len(jaw_src)-1):
                pt1 = tuple(map(int, jaw_src[i]))
                pt2 = tuple(map(int, jaw_src[i+1]))
                cv2.line(jaw_mask, pt1, pt2, 1.0, 20)
            jaw_mask = cv2.GaussianBlur(jaw_mask, (51, 51), 15)
            
            # åº”ç”¨å˜å½¢
            warped = cv2.remap(result, grid_x.astype(np.float32), grid_y.astype(np.float32), cv2.INTER_LINEAR)
            
            # ä½¿ç”¨å¹³æ»‘æ©ç è¿›è¡Œæ··åˆ
            jaw_mask = np.dstack([jaw_mask] * 3)
            result = (warped * jaw_mask + result * (1 - jaw_mask)).astype(np.uint8)

            # å¤„ç†çœ¼ç›
            eye_count = 0
            left_eye_indices = [36, 37, 38, 39, 40, 41]
            right_eye_indices = [42, 43, 44, 45, 46, 47]
            
            for eye_indices in [left_eye_indices, right_eye_indices]:
                try:
                    eye_points = landmarks[eye_indices]
                    
                    if len(eye_points) < 6 or not np.all(np.isfinite(eye_points)):
                        continue

                    center = np.mean(eye_points, axis=0)
                    if not np.all(np.isfinite(center)):
                        continue

                    x_diffs = eye_points[:, 0] - center[0]
                    y_diffs = eye_points[:, 1] - center[1]
                    
                    if not np.all(np.isfinite(x_diffs)) or not np.all(np.isfinite(y_diffs)):
                        continue

                    x_radius = float(np.max(np.abs(x_diffs))) * 3.0
                    y_radius = float(np.max(np.abs(y_diffs))) * 3.0

                    if x_radius < 1.0 or y_radius < 1.0:
                        x_radius = max(x_radius, 1.0)
                        y_radius = max(y_radius, 1.0)

                    dx = X - center[0]
                    dy = Y - center[1]
                    dist = np.sqrt((dx / x_radius) ** 2 + (dy / y_radius) ** 2)

                    pupil_radius = 0.2
                    mask = (dist <= 1.0) & (dist > pupil_radius)

                    if not np.any(mask):
                        continue

                    strength_x = np.zeros_like(dist)
                    strength_y = np.zeros_like(dist)
                    
                    mask_coef = (1 - (dist[mask] - pupil_radius) / (1.0 - pupil_radius))
                    strength_x[mask] = mask_coef * (eye_scale_x - 1) * 2.0
                    strength_y[mask] = mask_coef * (eye_scale_y - 1) * 2.0

                    map_x = X + dx * strength_x
                    map_y = Y + dy * strength_y

                    map_x = np.clip(map_x, 0, w-1)
                    map_y = np.clip(map_y, 0, h-1)

                    # åˆ›å»ºçœ¼ç›åŒºåŸŸçš„å¹³æ»‘æ©ç 
                    eye_mask = np.zeros_like(dist, dtype=np.float32)
                    eye_mask[mask] = 1.0
                    eye_mask = cv2.GaussianBlur(eye_mask, (31, 31), 10)
                    
                    # åº”ç”¨çœ¼ç›å˜å½¢
                    warped = cv2.remap(result, map_x.astype(np.float32), map_y.astype(np.float32), cv2.INTER_LINEAR)
                    
                    # ä½¿ç”¨å¹³æ»‘æ©ç è¿›è¡Œæ··åˆ
                    eye_mask = np.dstack([eye_mask] * 3)
                    result = (warped * eye_mask + result * (1 - eye_mask)).astype(np.uint8)
                    eye_count += 1

                except Exception as e:
                    print(f"å¤„ç†çœ¼ç›æ—¶å‡ºé”™: {str(e)}")
                    continue

            print(f"æˆåŠŸå¤„ç† {eye_count} ä¸ªçœ¼ç›")

        return result
    except Exception as e:
        print(f"å¤„ç†å›¾åƒæ—¶å‡ºé”™: {str(e)}")
        return img


def process_video_stream(video_frame, face_strength, eye_scale_x, eye_scale_y):
    """å¤„ç†å®æ—¶è§†é¢‘æµçš„å‡½æ•°ï¼ˆCPUç‰ˆæœ¬ï¼‰"""
    try:
        detector = dlib.get_frontal_face_detector()
        predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")

        if len(video_frame.shape) == 3 and video_frame.shape[2] == 3:
            frame_rgb = cv2.cvtColor(video_frame, cv2.COLOR_BGR2RGB)
        else:
            frame_rgb = video_frame

        faces = detector(frame_rgb, 1)
        if len(faces) == 0:
            return video_frame

        landmarks = predictor(frame_rgb, faces[0])
        processed_frame = process_image(frame_rgb, landmarks, face_strength, eye_scale_x, eye_scale_y)

        if len(video_frame.shape) == 3 and video_frame.shape[2] == 3:
            processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_RGB2BGR)

        return processed_frame

    except Exception as e:
        print(f"å¤„ç†è§†é¢‘å¸§æ—¶å‡ºé”™: {str(e)}")
        return video_frame


if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = FaceSwapApp()
    window.show()
    sys.exit(app.exec_())







